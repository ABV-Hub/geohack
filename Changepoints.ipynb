{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lasio\n",
    "import glob\n",
    "import seaborn\n",
    "from __future__ import division\n",
    "import cProfile\n",
    "\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l=lasio.read(r'D:\\geohack\\LAS\\5002310015.las') #reads the well log .las file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPT\t[F]\t\t1  DEPTH\n",
      "SP\t[MV]\t\t2\n",
      "SN\t[OHMM]\t\t3\n",
      "ILD\t[OHMM]\t\t4\n",
      "CILD\t[OHMM]\t\t5\n",
      "DT\t[US/F]\t\t6\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "for curve in l.curves:\n",
    "    print(\"%s\\t[%s]\\t%s\\t%s\" % (curve.mnemonic, curve.unit, curve.value, curve.descr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gr = pd.Series(l['ILD']).fillna(value=0)\n",
    "depth = pd.Series(l['DEPT']).fillna(value=0)\n",
    "df = pd.DataFrame()\n",
    "df['depth'] = l['DEPT']\n",
    "df['sn'] = l['ILD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use scipy logsumexp().\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "from scipy.special import gammaln, multigammaln\n",
    "from scipy.misc import comb\n",
    "from decorator import decorator\n",
    "\n",
    "# This makes the code compatible with Python 3\n",
    "# without causing performance hits on Python 2\n",
    "try:\n",
    "    xrange\n",
    "except NameError:\n",
    "    xrange = range\n",
    "\n",
    "\n",
    "try:\n",
    "    from sselogsumexp import logsumexp\n",
    "except ImportError:\n",
    "    from scipy.misc import logsumexp\n",
    "    print(\"Use scipy logsumexp().\")\n",
    "else:\n",
    "    print(\"Use SSE accelerated logsumexp().\")\n",
    "\n",
    "\n",
    "def _dynamic_programming(f, *args, **kwargs):\n",
    "    if f.data is None:\n",
    "        f.data = args[0]\n",
    "\n",
    "    if not np.array_equal(f.data, args[0]):\n",
    "        f.cache = {}\n",
    "        f.data = args[0]\n",
    "\n",
    "    try:\n",
    "        f.cache[args[1:3]]\n",
    "    except KeyError:\n",
    "        f.cache[args[1:3]] = f(*args, **kwargs)\n",
    "    return f.cache[args[1:3]]\n",
    "\n",
    "def dynamic_programming(f):\n",
    "    f.cache = {}\n",
    "    f.data = None\n",
    "    return decorator(_dynamic_programming, f)\n",
    "\n",
    "\n",
    "def offline_changepoint_detection(data, prior_func,\n",
    "                                  observation_log_likelihood_function,\n",
    "                                  truncate=-np.inf):\n",
    "    \"\"\"Compute the likelihood of changepoints on data.\n",
    "    Keyword arguments:\n",
    "    data                                -- the time series data\n",
    "    prior_func                          -- a function given the likelihood of a changepoint given the distance to the last one\n",
    "    observation_log_likelihood_function -- a function giving the log likelihood\n",
    "                                           of a data part\n",
    "    truncate                            -- the cutoff probability 10^truncate to stop computation for that changepoint log likelihood\n",
    "    P                                   -- the likelihoods if pre-computed\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(data)\n",
    "    Q = np.zeros((n,))\n",
    "    g = np.zeros((n,))\n",
    "    G = np.zeros((n,))\n",
    "    P = np.ones((n, n)) * -np.inf\n",
    "\n",
    "    # save everything in log representation\n",
    "    for t in range(n):\n",
    "        g[t] = np.log(prior_func(t))\n",
    "        if t == 0:\n",
    "            G[t] = g[t]\n",
    "        else:\n",
    "            G[t] = np.logaddexp(G[t-1], g[t])\n",
    "\n",
    "    P[n-1, n-1] = observation_log_likelihood_function(data, n-1, n)\n",
    "    Q[n-1] = P[n-1, n-1]\n",
    "\n",
    "    for t in reversed(range(n-1)):\n",
    "        P_next_cp = -np.inf  # == log(0)\n",
    "        for s in range(t, n-1):\n",
    "            P[t, s] = observation_log_likelihood_function(data, t, s+1)\n",
    "\n",
    "            # compute recursion\n",
    "            summand = P[t, s] + Q[s + 1] + g[s + 1 - t]\n",
    "            P_next_cp = np.logaddexp(P_next_cp, summand)\n",
    "\n",
    "            # truncate sum to become approx. linear in time (see\n",
    "            # Fearnhead, 2006, eq. (3))\n",
    "            if summand - P_next_cp < truncate:\n",
    "                break\n",
    "\n",
    "        P[t, n-1] = observation_log_likelihood_function(data, t, n)\n",
    "\n",
    "        # (1 - G) is numerical stable until G becomes numerically 1\n",
    "        if G[n-1-t] < -1e-15:  # exp(-1e-15) = .99999...\n",
    "            antiG = np.log(1 - np.exp(G[n-1-t]))\n",
    "        else:\n",
    "            # (1 - G) is approx. -log(G) for G close to 1\n",
    "            antiG = np.log(-G[n-1-t])\n",
    "\n",
    "        Q[t] = np.logaddexp(P_next_cp, P[t, n-1] + antiG)\n",
    "\n",
    "    Pcp = np.ones((n-1, n-1)) * -np.inf\n",
    "    for t in range(n-1):\n",
    "        Pcp[0, t] = P[0, t] + Q[t + 1] + g[t] - Q[0]\n",
    "        if np.isnan(Pcp[0, t]):\n",
    "            Pcp[0, t] = -np.inf\n",
    "    for j in range(1, n-1):\n",
    "        for t in range(j, n-1):\n",
    "            tmp_cond = Pcp[j-1, j-1:t] + P[j:t+1, t] + Q[t + 1] + g[0:t-j+1] - Q[j:t+1]\n",
    "            Pcp[j, t] = logsumexp(tmp_cond.astype(np.float32))\n",
    "            if np.isnan(Pcp[j, t]):\n",
    "                Pcp[j, t] = -np.inf\n",
    "\n",
    "    return Q, P, Pcp\n",
    "\n",
    "@dynamic_programming\n",
    "def gaussian_obs_log_likelihood(data, t, s):\n",
    "    s += 1\n",
    "    n = s - t\n",
    "    mean = data[t:s].sum(0) / n\n",
    "\n",
    "    muT = (n * mean) / (1 + n)\n",
    "    nuT = 1 + n\n",
    "    alphaT = 1 + n / 2\n",
    "    betaT = 1 + 0.5 * ((data[t:s] - mean) ** 2).sum(0) + ((n)/(1 + n)) * (mean**2 / 2)\n",
    "    scale = (betaT*(nuT + 1))/(alphaT * nuT)\n",
    "\n",
    "    # splitting the PDF of the student distribution up is /much/ faster.\n",
    "    # (~ factor 20) using sum over for loop is even more worthwhile\n",
    "    prob = np.sum(np.log(1 + (data[t:s] - muT)**2/(nuT * scale)))\n",
    "    lgA = gammaln((nuT + 1) / 2) - np.log(np.sqrt(np.pi * nuT * scale)) - gammaln(nuT/2)\n",
    "\n",
    "    return np.sum(n * lgA - (nuT + 1)/2 * prob)\n",
    "\n",
    "def ifm_obs_log_likelihood(data, t, s):\n",
    "    '''Independent Features model from xuan et al'''\n",
    "    s += 1\n",
    "    n = s - t\n",
    "    x = data[t:s]\n",
    "    if len(x.shape)==2:\n",
    "        d = x.shape[1]\n",
    "    else:\n",
    "        d = 1\n",
    "        x = np.atleast_2d(x).T\n",
    "\n",
    "    N0 = d          # weakest prior we can use to retain proper prior\n",
    "    V0 = np.var(x)\n",
    "    Vn = V0 + (x**2).sum(0)\n",
    "\n",
    "    # sum over dimension and return (section 3.1 from Xuan paper):\n",
    "    return d*( -(n/2)*np.log(np.pi) + (N0/2)*np.log(V0) - \\\n",
    "        gammaln(N0/2) + gammaln((N0+n)/2) ) - \\\n",
    "        ( ((N0+n)/2)*np.log(Vn) ).sum(0)\n",
    "\n",
    "def fullcov_obs_log_likelihood(data, t, s):\n",
    "    '''Full Covariance model from xuan et al'''\n",
    "    s += 1\n",
    "    n = s - t\n",
    "    x = data[t:s]\n",
    "    if len(x.shape)==2:\n",
    "        dim = x.shape[1]\n",
    "    else:\n",
    "        dim = 1\n",
    "        x = np.atleast_2d(x).T\n",
    "\n",
    "    N0 = dim          # weakest prior we can use to retain proper prior\n",
    "    V0 = np.var(x)*np.eye(dim)\n",
    "    \n",
    "    # Improvement over np.outer\n",
    "    # http://stackoverflow.com/questions/17437523/python-fast-way-to-sum-outer-products\n",
    "    # Vn = V0 + np.array([np.outer(x[i], x[i].T) for i in xrange(x.shape[0])]).sum(0)\n",
    "    Vn = V0 + np.einsum('ij,ik->jk', x, x)\n",
    "\n",
    "    # section 3.2 from Xuan paper:\n",
    "    return -(dim*n/2)*np.log(np.pi) + (N0/2)*np.linalg.slogdet(V0)[1] - \\\n",
    "        multigammaln(N0/2,dim) + multigammaln((N0+n)/2,dim) - \\\n",
    "        ((N0+n)/2)*np.linalg.slogdet(Vn)[1]\n",
    "\n",
    "def const_prior(r, l):\n",
    "    return 1/(l)\n",
    "\n",
    "def geometric_prior(t, p):\n",
    "    return p * ((1 - p) ** (t - 1))\n",
    "\n",
    "def neg_binominal_prior(t, k, p):\n",
    "    return comb(t - k, k - 1) * p ** k * (1 - p) ** (t - k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = df.sn.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "Q, P, Pcp = offline_changepoint_detection(data, partial(const_prior, l=(len(data)+1)), gaussian_obs_log_likelihood, truncate=-40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
