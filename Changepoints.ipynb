{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lasio\n",
    "import glob\n",
    "import seaborn\n",
    "from __future__ import division\n",
    "import cProfile\n",
    "\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l=lasio.read(r'D:\\geohack\\LAS\\5030120001.las') #reads the well log .las file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPT\t[F]\t\t1  DEPTH\n",
      "SP\t[MV]\t\t2\n",
      "ILD\t[OHMM]\t\t3\n",
      "ILM\t[OHMM]\t\t4\n",
      "LL8\t[OHMM]\t\t5\n",
      "MSFL\t[OHMM]\t\t6\n",
      "LLD\t[OHMM]\t\t7\n",
      "LLS\t[OHMM]\t\t8\n",
      "CALI\t[IN]\t\t9\n",
      "RHOB\t[G/C3]\t\t10\n",
      "DRHO\t[G/C3]\t\t11\n",
      "NPHI\t[%]\t\t12\n",
      "GR\t[GAPI]\t\t13\n",
      "DT\t[US/F]\t\t14\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "for curve in l.curves:\n",
    "    print(\"%s\\t[%s]\\t%s\\t%s\" % (curve.mnemonic, curve.unit, curve.value, curve.descr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>depth</th>\n",
       "      <th>sn</th>\n",
       "      <th>sp</th>\n",
       "      <th>ild</th>\n",
       "      <th>dt</th>\n",
       "      <th>gr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>104.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.4842</td>\n",
       "      <td>28.4006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104.5</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.5375</td>\n",
       "      <td>28.2358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>105.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.0702</td>\n",
       "      <td>28.0711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>105.5</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95.4594</td>\n",
       "      <td>27.9736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>106.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.6074</td>\n",
       "      <td>27.9059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   depth    sn  sp  ild       dt       gr\n",
       "0  104.0  None NaN  NaN  99.4842  28.4006\n",
       "1  104.5  None NaN  NaN  99.5375  28.2358\n",
       "2  105.0  None NaN  NaN  96.0702  28.0711\n",
       "3  105.5  None NaN  NaN  95.4594  27.9736\n",
       "4  106.0  None NaN  NaN  96.6074  27.9059"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df['depth'] = l['DEPT']\n",
    "df['sn'] = l['SN']\n",
    "df['sp'] = l['SP']\n",
    "df['ild'] = l['ILD']\n",
    "df['dt'] = l['DT']\n",
    "df['gr'] = l['GR']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use scipy logsumexp().\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "from scipy.special import gammaln, multigammaln\n",
    "from scipy.misc import comb\n",
    "from decorator import decorator\n",
    "\n",
    "# This makes the code compatible with Python 3\n",
    "# without causing performance hits on Python 2\n",
    "try:\n",
    "    xrange\n",
    "except NameError:\n",
    "    xrange = range\n",
    "\n",
    "\n",
    "try:\n",
    "    from sselogsumexp import logsumexp\n",
    "except ImportError:\n",
    "    from scipy.special import logsumexp\n",
    "    print(\"Use scipy logsumexp().\")\n",
    "else:\n",
    "    print(\"Use SSE accelerated logsumexp().\")\n",
    "\n",
    "\n",
    "def _dynamic_programming(f, *args, **kwargs):\n",
    "    if f.data is None:\n",
    "        f.data = args[0]\n",
    "\n",
    "    if not np.array_equal(f.data, args[0]):\n",
    "        f.cache = {}\n",
    "        f.data = args[0]\n",
    "\n",
    "    try:\n",
    "        f.cache[args[1:3]]\n",
    "    except KeyError:\n",
    "        f.cache[args[1:3]] = f(*args, **kwargs)\n",
    "    return f.cache[args[1:3]]\n",
    "\n",
    "def dynamic_programming(f):\n",
    "    f.cache = {}\n",
    "    f.data = None\n",
    "    return decorator(_dynamic_programming, f)\n",
    "\n",
    "\n",
    "def offline_changepoint_detection(data, prior_func,\n",
    "                                  observation_log_likelihood_function,\n",
    "                                  truncate=-np.inf):\n",
    "    \"\"\"Compute the likelihood of changepoints on data.\n",
    "    Keyword arguments:\n",
    "    data                                -- the time series data\n",
    "    prior_func                          -- a function given the likelihood of a changepoint given the distance to the last one\n",
    "    observation_log_likelihood_function -- a function giving the log likelihood\n",
    "                                           of a data part\n",
    "    truncate                            -- the cutoff probability 10^truncate to stop computation for that changepoint log likelihood\n",
    "    P                                   -- the likelihoods if pre-computed\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(data)\n",
    "    Q = np.zeros((n,))\n",
    "    g = np.zeros((n,))\n",
    "    G = np.zeros((n,))\n",
    "    P = np.ones((n, n)) * -np.inf\n",
    "\n",
    "    # save everything in log representation\n",
    "    for t in range(n):\n",
    "        g[t] = np.log(prior_func(t))\n",
    "        if t == 0:\n",
    "            G[t] = g[t]\n",
    "        else:\n",
    "            G[t] = np.logaddexp(G[t-1], g[t])\n",
    "\n",
    "    P[n-1, n-1] = observation_log_likelihood_function(data, n-1, n)\n",
    "    Q[n-1] = P[n-1, n-1]\n",
    "\n",
    "    for t in reversed(range(n-1)):\n",
    "        P_next_cp = -np.inf  # == log(0)\n",
    "        for s in range(t, n-1):\n",
    "            P[t, s] = observation_log_likelihood_function(data, t, s+1)\n",
    "\n",
    "            # compute recursion\n",
    "            summand = P[t, s] + Q[s + 1] + g[s + 1 - t]\n",
    "            P_next_cp = np.logaddexp(P_next_cp, summand)\n",
    "\n",
    "            # truncate sum to become approx. linear in time (see\n",
    "            # Fearnhead, 2006, eq. (3))\n",
    "            if summand - P_next_cp < truncate:\n",
    "                break\n",
    "\n",
    "        P[t, n-1] = observation_log_likelihood_function(data, t, n)\n",
    "\n",
    "        # (1 - G) is numerical stable until G becomes numerically 1\n",
    "        if G[n-1-t] < -1e-15:  # exp(-1e-15) = .99999...\n",
    "            antiG = np.log(1 - np.exp(G[n-1-t]))\n",
    "        else:\n",
    "            # (1 - G) is approx. -log(G) for G close to 1\n",
    "            antiG = np.log(-G[n-1-t])\n",
    "\n",
    "        Q[t] = np.logaddexp(P_next_cp, P[t, n-1] + antiG)\n",
    "\n",
    "    Pcp = np.ones((n-1, n-1)) * -np.inf\n",
    "    for t in range(n-1):\n",
    "        Pcp[0, t] = P[0, t] + Q[t + 1] + g[t] - Q[0]\n",
    "        if np.isnan(Pcp[0, t]):\n",
    "            Pcp[0, t] = -np.inf\n",
    "    for j in range(1, n-1):\n",
    "        for t in range(j, n-1):\n",
    "            tmp_cond = Pcp[j-1, j-1:t] + P[j:t+1, t] + Q[t + 1] + g[0:t-j+1] - Q[j:t+1]\n",
    "            Pcp[j, t] = logsumexp(tmp_cond.astype(np.float32))\n",
    "            if np.isnan(Pcp[j, t]):\n",
    "                Pcp[j, t] = -np.inf\n",
    "\n",
    "    return Q, P, Pcp\n",
    "\n",
    "@dynamic_programming\n",
    "def gaussian_obs_log_likelihood(data, t, s):\n",
    "    s += 1\n",
    "    n = s - t\n",
    "    mean = data[t:s].sum(0) / n\n",
    "\n",
    "    muT = (n * mean) / (1 + n)\n",
    "    nuT = 1 + n\n",
    "    alphaT = 1 + n / 2\n",
    "    betaT = 1 + 0.5 * ((data[t:s] - mean) ** 2).sum(0) + ((n)/(1 + n)) * (mean**2 / 2)\n",
    "    scale = (betaT*(nuT + 1))/(alphaT * nuT)\n",
    "\n",
    "    # splitting the PDF of the student distribution up is /much/ faster.\n",
    "    # (~ factor 20) using sum over for loop is even more worthwhile\n",
    "    prob = np.sum(np.log(1 + (data[t:s] - muT)**2/(nuT * scale)))\n",
    "    lgA = gammaln((nuT + 1) / 2) - np.log(np.sqrt(np.pi * nuT * scale)) - gammaln(nuT/2)\n",
    "\n",
    "    return np.sum(n * lgA - (nuT + 1)/2 * prob)\n",
    "\n",
    "def ifm_obs_log_likelihood(data, t, s):\n",
    "    '''Independent Features model from xuan et al'''\n",
    "    s += 1\n",
    "    n = s - t\n",
    "    x = data[t:s]\n",
    "    if len(x.shape)==2:\n",
    "        d = x.shape[1]\n",
    "    else:\n",
    "        d = 1\n",
    "        x = np.atleast_2d(x).T\n",
    "\n",
    "    N0 = d          # weakest prior we can use to retain proper prior\n",
    "    V0 = np.var(x)\n",
    "    Vn = V0 + (x**2).sum(0)\n",
    "\n",
    "    # sum over dimension and return (section 3.1 from Xuan paper):\n",
    "    return d*( -(n/2)*np.log(np.pi) + (N0/2)*np.log(V0) - \\\n",
    "        gammaln(N0/2) + gammaln((N0+n)/2) ) - \\\n",
    "        ( ((N0+n)/2)*np.log(Vn) ).sum(0)\n",
    "\n",
    "def fullcov_obs_log_likelihood(data, t, s):\n",
    "    '''Full Covariance model from xuan et al'''\n",
    "    s += 1\n",
    "    n = s - t\n",
    "    x = data[t:s]\n",
    "    if len(x.shape)==2:\n",
    "        dim = x.shape[1]\n",
    "    else:\n",
    "        dim = 1\n",
    "        x = np.atleast_2d(x).T\n",
    "\n",
    "    N0 = dim          # weakest prior we can use to retain proper prior\n",
    "    V0 = np.var(x)*np.eye(dim)\n",
    "    \n",
    "    # Improvement over np.outer\n",
    "    # http://stackoverflow.com/questions/17437523/python-fast-way-to-sum-outer-products\n",
    "    # Vn = V0 + np.array([np.outer(x[i], x[i].T) for i in xrange(x.shape[0])]).sum(0)\n",
    "    Vn = V0 + np.einsum('ij,ik->jk', x, x)\n",
    "\n",
    "    # section 3.2 from Xuan paper:\n",
    "    return -(dim*n/2)*np.log(np.pi) + (N0/2)*np.linalg.slogdet(V0)[1] - \\\n",
    "        multigammaln(N0/2,dim) + multigammaln((N0+n)/2,dim) - \\\n",
    "        ((N0+n)/2)*np.linalg.slogdet(Vn)[1]\n",
    "\n",
    "def const_prior(r, l):\n",
    "    return 1/(l)\n",
    "\n",
    "def geometric_prior(t, p):\n",
    "    return p * ((1 - p) ** (t - 1))\n",
    "\n",
    "def neg_binominal_prior(t, k, p):\n",
    "    return comb(t - k, k - 1) * p ** k * (1 - p) ** (t - k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 28.4006,  28.2358,  28.0711, ...,  66.176 ,  65.3577,  65.328 ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.gr.values[0:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import binarize\n",
    "from functools import partial\n",
    "\n",
    "data = df.gr.values[0:5000]\n",
    "Q, P, Pcp = offline_changepoint_detection(data, partial(const_prior, l=(len(data)+1)), gaussian_obs_log_likelihood, truncate=-90)\n",
    "picks = np.exp(Pcp).sum(0)\n",
    "picks = picks.reshape(1,-1)\n",
    "bin_picks = np.squeeze(binarize(picks, threshold = 0.3))*45\n",
    "df['gr_top']=np.append(bin_picks, 45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['interval'] = np.nan\n",
    "intervals = df[df['gr_top']>1].depth.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "intervals = df[df['sn_top']>1].depth.values\n",
    "for i in range(len(intervals)-1):\n",
    "    ind = df[(df['depth']>intervals[i]) & (df['depth']<intervals[i+1])].index\n",
    "    df.loc[ind, 'interval'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(df.depth.values, df.interval.values)\n",
    "plt.plot(df.depth.values, df.sn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv(r'D:/ak_out/5002320003.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(df['gr'], df['depth']*-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
